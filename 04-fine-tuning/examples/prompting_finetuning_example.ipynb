{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zZ2JPmfYg5yc"
      },
      "source": [
        "## FINE-TUNING\n",
        "Here we have an example of fine-tunning a pre-trained model to create prompts. We will ask the model to act like \"a chemistry expert\" and create prompts."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WGSGcpE2hAwk"
      },
      "source": [
        "### 1) SET UP THE ENVIROMENT\n",
        "\n",
        "Install required libraries.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GmRcX20ChHXT"
      },
      "outputs": [],
      "source": [
        "!pip install -q peft==0.10.0\n",
        "!pip install -q datasets==2.19.0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fJp7B3GIjZ_w"
      },
      "source": [
        "### 2) LOAD THE MODEL\n",
        "\n",
        "We are using Bloom, one of the smallest and smarters models avaliable to be trained with the PEFT library.\n",
        "\n",
        "We can choose between the Bloom \"big\" an \"small\" models. In this case we chose the small one (bigscience/bloomz-560m), instead of the big one (bigscience/bloom-1b1); which would be computationally expensive. With this smaller model, is possible to spend less time trainig and avoid memory problems in Colab."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-7jqfRm3hfoN"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "model_name = \"bigscience/bloomz-560m\"\n",
        "device = \"cpu\" #\"cuda\" for NVIDIA GPUs, or \"cpu\" for no GPU.\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name,\n",
        "                                                        device_map = device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ehoh1L7llTtq"
      },
      "source": [
        "> Specifying \"device\" we make sure that the model is loaded on the appropriate hardware for performance (GPU if available, CPU otherwise), making processing more efficient."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d1KrohRDhpgV"
      },
      "source": [
        "### 3) INFERENCE WITH THE PRE-TRAINED MODEL\n",
        "\n",
        "This step is just to test the performance of the pre-trained model without fine-tuning, to see if something changes after the fine-tuning process.\n",
        "\n",
        "We want the model to generate a prompt acting as a chemistry expert. In this step of the process, we will ask the model to do it. Then, after fine-tuning, we'll ask the same and see the differences between the two generated prompts."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3kAmv1O_Uewx"
      },
      "source": [
        "\n",
        "The next function, get_outputs, generates text from a language model given input tokens.\n",
        "\n",
        "Parameters:\n",
        "\n",
        "- model: the pre-trained model used for text generation.\n",
        "\n",
        "- inputs: tokenized input (includes input_ids and attention_mask).\n",
        "\n",
        "- max_new_tokens: maximum number of new tokens the model can generate.\n",
        "\n",
        "- repetition_penalty: penalty used to avoid repetition.\n",
        "- early_stopping: Allows the model to stop generating text before reaching max_new_tokens.\n",
        "\n",
        "- eos_token_id: end-of-sequence token to stop generation.\n",
        "\n",
        "The function returns the generated token sequence based on the input and model configuration."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "atC73iGYh8yN"
      },
      "outputs": [],
      "source": [
        "def get_outputs(model, inputs, max_new_tokens=100):\n",
        "    outputs = model.generate(\n",
        "        input_ids=inputs[\"input_ids\"],\n",
        "        attention_mask=inputs[\"attention_mask\"],\n",
        "        max_new_tokens=max_new_tokens,\n",
        "        repetition_penalty=1.5,\n",
        "        early_stopping=True,\n",
        "        eos_token_id=tokenizer.eos_token_id\n",
        "    )\n",
        "    return outputs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VgZVLkzIZQnN"
      },
      "source": [
        "Now is time to inference the original model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3BAYg7czFYeK"
      },
      "outputs": [],
      "source": [
        "input_sentences = tokenizer(\"I want you to act as chemistry expert. \", return_tensors=\"pt\")\n",
        "outputs_sentence = get_outputs(model, input_sentences.to(device), max_new_tokens=50)\n",
        "\n",
        "print(tokenizer.batch_decode(outputs_sentence, skip_special_tokens=True))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UBoK0Ec5iPaz"
      },
      "source": [
        "### 4) LOAD AND PREPARE THE DATASET\n",
        "\n",
        "For preparing the model to act well as a chemistry expert, we'll \"give\" the model a dataset that contains prompts to be used with LLMs. Thereby, the model can learn with the example prompts that are contained in the dataset.\n",
        "\n",
        "The dataset is\n",
        "\n",
        "https://huggingface.co/datasets/fka/awesome-chatgpt-prompts"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zj2KV73WRQOW"
      },
      "source": [
        "In this cells we:\n",
        "\n",
        "- load the dataset\n",
        "- tokenize the data using the map function. This map function applies a tokenizer to the \"prompt\" column of the dataset, batch-processing multiple samples at once.\n",
        "- select a subset (in this case we select the first 50 samples from the \"train\" split of the dataset)\n",
        "- remove the 'act' colum from the dataset (we just want the prompts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-pZtY5GWSJyG"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "dataset = \"fka/awesome-chatgpt-prompts\"\n",
        "\n",
        "data = load_dataset(dataset)\n",
        "display(data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "joWfxsvviVpm"
      },
      "outputs": [],
      "source": [
        "data = data.map(lambda samples: tokenizer(samples[\"prompt\"]), batched=True)\n",
        "train_sample = data[\"train\"].select(range(50))\n",
        "\n",
        "train_sample = train_sample.remove_columns('act')\n",
        "\n",
        "display(train_sample)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qbPpvQx7iZK1"
      },
      "source": [
        "### 5) SET UP TRAINING CONFIGURATION FOR THE FINE-TUNING PROCESS\n",
        "\n",
        "#### 1. Create a LoRa config.\n",
        "\n",
        "This code block defines a LoRa configuration using the peft library. Here's an explanation the paramenters:\n",
        "\n",
        "- r: Controls the rank, or the number of adaptation dimensions. As bigger the R, bigger the parameters there are to train. This means that increasing r results in more expressive power but also requires more computational resources for training.\n",
        "\n",
        "- lora_alpha: a multiplier controlling the overall strength of connections within a Neural Network. Smaller values provide more control over learning rates. Typically set at 1.\n",
        "\n",
        "- target_modules: Specifies the layers/modules to be adapted. Each model have a specific value that are targeted to efficiently modify key parts of the model during the lora adaptation process. You can check some of them here: https://github.com/huggingface/peft/blob/39ef2546d5d9b8f5f8a7016ec10657887a867041/src/peft/utils/other.py#L220\n",
        "\n",
        "- lora_dropout: helps to avoid overfitting.\n",
        "\n",
        "- bias=\"lora_only\": ensures that only the LoRa-specific parameters are fine-tuned during training, not the entire model. Controls whether the bias term is adjusted alongside the model weights during training. When set to \"lora_only,\" the bias remains unaffected, focusing the fine-tuning on LoRa parameters for a more efficient and lightweight training process.\n",
        "\n",
        "- task_type=\"CAUSAL_LM\": specifies the type of task for the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EOkMY2LfifzM"
      },
      "outputs": [],
      "source": [
        "import peft\n",
        "from peft import LoraConfig, get_peft_model, PeftModel\n",
        "\n",
        "lora_config = LoraConfig(\n",
        "    r=4,\n",
        "    lora_alpha=1,\n",
        "    target_modules=[\"query_key_value\"],\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"lora_only\",\n",
        "    task_type=\"CAUSAL_LM\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ILre9S7irlx"
      },
      "source": [
        "#### 2. Create the PEFT model\n",
        "\n",
        "Build the fine tuned model using Lora with the chosen configuration  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_jhf4J4sivlH"
      },
      "outputs": [],
      "source": [
        "peft_model = get_peft_model(model, lora_config)\n",
        "print(peft_model.print_trainable_parameters())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5A9hn-3oi1eG"
      },
      "source": [
        "We can see that the number of trainable parameters is really small compared with the total number of parameters in the pre-trained model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n9jIYL63Xlhg"
      },
      "source": [
        "Now we create a directory to contain the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0TS8LIP5i2S2"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "working_dir = './'\n",
        "\n",
        "output_directory = os.path.join(working_dir, \"peft_lab_outputs\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "umTmB4r6aoWT"
      },
      "source": [
        "#### 3. Create the TrainingArgs\n",
        "\n",
        "In this cell we set the configuration for model training, incluiding the hyperparameters, with \"TrainingArguments\"."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j39GU6nSYeOp"
      },
      "source": [
        "- output_dir: specifies where to save the model's outputs (the output directory that we create in the last cell)\n",
        "\n",
        "- auto_find_batch_size=True: to automatically determine a batch size that fits the data and system's memory.\n",
        "\n",
        "- learning_rate=3e-2: determines how much to adjust the model's weights with respect to the loss gradient during training. A small learning rate makes the model learn slowly and converge more precisely, but it may take longer. A large learning rate allows the model to learn faster, but with less precision.\n",
        "\n",
        "- num_train_epochs=2: number of times to \"loop over\" the training dataset.\n",
        "\n",
        "- use_cpu=False: specifies if we want to use the CPU (we set to False, for using a GPU if available)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4FiDcGBHi7z9"
      },
      "outputs": [],
      "source": [
        "import transformers\n",
        "from transformers import TrainingArguments, Trainer\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=output_directory,\n",
        "    auto_find_batch_size=True,\n",
        "    learning_rate= 3e-2,\n",
        "    num_train_epochs=2,\n",
        "    use_cpu=False\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rgoGQoXkjGNz"
      },
      "source": [
        "### 6) TRAIN THE MODEL\n",
        "\n",
        "(The fine-tuning happens here)\n",
        "\n",
        "For training the model we need:\n",
        "- the PEFT model\n",
        "- the training_args\n",
        "- the dataset\n",
        "- the result of DataCollator, the dataset ready to be processed in blocks"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "A wandb API key may be requested before starting training. Weights & Biases (wandb), request API keys to log the training metrics and some details to the wandb platform, which provides visualization and tracking tools to help monitorizing and analyzing ML projects.\n",
        "\n",
        "In our code, we didn't explicitly added wandb, but some machine learning libraries like Hugging Face's Transformers, automatically integrate with wandb.\n",
        "\n",
        "For this case we don't need/want these tracking features, so we will disable wandb. We can also make sure that wandb is not being called during the training process removing explicitly the wandbcallback before starting training."
      ],
      "metadata": {
        "id": "7mCuwUh2pula"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6h7wzQZr3EWr"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import transformers\n",
        "\n",
        "os.environ[\"WANDB_MODE\"] = \"disabled\"  # Disable W&B globally\n",
        "\n",
        "# Remove potential existing WandbCallbacks before training\n",
        "# trainer.remove_callback(transformers.integrations.WandbCallback)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HGKgENxvjQo-"
      },
      "outputs": [],
      "source": [
        "trainer = Trainer(\n",
        "    model=peft_model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_sample,\n",
        "    data_collator=transformers.DataCollatorForLanguageModeling(\n",
        "        tokenizer,\n",
        "        mlm=False)\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jpi7ighTwRi3"
      },
      "outputs": [],
      "source": [
        "trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z1DdxF9naJXt"
      },
      "source": [
        "Now we save and then \"load again\" the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2bONmxdIjUZq"
      },
      "outputs": [],
      "source": [
        "peft_model_path = os.path.join(output_directory, f\"lora_model\")\n",
        "\n",
        "trainer.model.save_pretrained(peft_model_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BX6ihR_4jVlu"
      },
      "outputs": [],
      "source": [
        "loaded_model = PeftModel.from_pretrained(model,\n",
        "                                        peft_model_path,\n",
        "                                        is_trainable=False)\n",
        "loaded_model.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rhu8qAzMjdv6"
      },
      "source": [
        "### 7) INFERENCE WITH THE FINE-TUNED MODEL\n",
        "\n",
        "Now, we ask the same to the model: create a prompt acting as a chemistry expert. Thereby, we can see the differences between the model's answers before and after the fine-tuning."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yGA1e3YSjhzh"
      },
      "outputs": [],
      "source": [
        "input_sentences = tokenizer(\"I want you to act as a chemistry expert.\", return_tensors=\"pt\")\n",
        "finetuned_outputs_sentence = get_outputs(loaded_model,\n",
        "                                          input_sentences.to(device),\n",
        "                                          max_new_tokens=50)\n",
        "\n",
        "print(tokenizer.batch_decode(finetuned_outputs_sentence, skip_special_tokens=True))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fb3XVlbna9BB"
      },
      "source": [
        "### 8) EXAMINATING AND COMPARING THE RESULTS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SoOnCkcHODt7"
      },
      "outputs": [],
      "source": [
        "print('COMPARING THE TWO ANSWERS:\\n')\n",
        "print('Pre-trained model: \\n')\n",
        "display(tokenizer.batch_decode(outputs_sentence, skip_special_tokens=True))\n",
        "print('\\n')\n",
        "print('Fine-tuned model: \\n')\n",
        "display(tokenizer.batch_decode(finetuned_outputs_sentence, skip_special_tokens=True))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gVZshclfbfcN"
      },
      "source": [
        "Despite the fact that the model is trained with minimal resources to be computationally efficient,, the difference between the pre-trained model and the fine-tuned model responses is clear in how they handle the same task.\n",
        "\n",
        "- The pre-trained model provides a general response without specific adjustments for chemistry.\n",
        "\n",
        "- The fine-tuned model generates a more domain-specific answer by understanding the context of chemistry and adding the response. The answer can include chemical-related language."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}