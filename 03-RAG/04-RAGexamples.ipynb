{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# RAG EXAMPLES"
      ],
      "metadata": {
        "id": "X-Rwhk0F31CR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this notebook, we show how to use OpenAI's API to interact with a Large Language Model (LLM) for generating text based on various prompts.\n",
        "\n",
        "Each example explores a different way to add content and enrich the model's responses.\n",
        "\n",
        "- **First example:** we ask a question without any context, allowing the model to respond solely from its own knowledge base.\n",
        "- **Second example:** introduces a document-based retrieval system to search for relevant information on-the-fly, allowing the model to answer with document-specific context.\n",
        "- **Third example:** demonstrates a direct connection to an operational database, showcasing how RAG can seamlessly integrate structured data sources, like SQL databases, to answer specific questions with dynamic, up-to-date information.\n",
        "\n",
        "By the end of this notebook, you will see how to integrate yout LLM with contextual data to produce more relevant, accurate, and tailored outputs in real-world applications."
      ],
      "metadata": {
        "id": "o1UgJxbL3CjI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setting up the environment:\n",
        "\n",
        "We'll instal the OpenAI 0.28 version because some of the example code uses some methods like openai.Completion and openai.ChatCompletion that are no longer supported in later versions (1.0 and above). The updated versions of the OpenAI library introduced changes in how the API is accessed and organized, requiring different syntax and method names.\n"
      ],
      "metadata": {
        "id": "eNtdvPM74gS1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q openai==0.28"
      ],
      "metadata": {
        "id": "eZXygRjo3VMB"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip show openai"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F1DVug2j3KfS",
        "outputId": "f9a1b40a-7141-4e02-ff19-c3ba44fd6522"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Name: openai\n",
            "Version: 0.28.0\n",
            "Summary: Python client library for the OpenAI API\n",
            "Home-page: https://github.com/openai/openai-python\n",
            "Author: OpenAI\n",
            "Author-email: support@openai.com\n",
            "License: \n",
            "Location: /usr/local/lib/python3.10/dist-packages\n",
            "Requires: aiohttp, requests, tqdm\n",
            "Required-by: \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "import os\n",
        "from getpass import getpass"
      ],
      "metadata": {
        "id": "zr9cAvPS0-X0"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set your API key\n",
        "\n",
        "openai.api_key = \"<YOUR_API_KEY_HERE>\"\n"
      ],
      "metadata": {
        "id": "wMzfhrah8Lu-"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. FIRST EXAMPLE: Asking the model without context\n",
        "\n",
        "We start asking the model a question directly, without any additional support from a retrieval system. Thereby, the model will answer based on its own pre-trained knowledge to generate a response, without adding external context or data to assist.\n"
      ],
      "metadata": {
        "id": "i7aIF3e94T8J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.1. First Test:\n",
        "\n",
        "Let’s now proceed to using the Chat Completions API by providing it with an input prompt, and in this example, we use Hello!"
      ],
      "metadata": {
        "id": "OIvNmJLc4I1T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the user prompt message\n",
        "prompt = \"Hello!\"\n",
        "# Create a chatbot using ChatCompletion.create() function\n",
        "completion = openai.ChatCompletion.create(\n",
        "  # Use GPT 3.5 as the LLM\n",
        "  model=\"gpt-3.5-turbo\",\n",
        "  # Pre-define conversation messages for the possible roles\n",
        "  messages=[\n",
        "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "    {\"role\": \"user\", \"content\": prompt}\n",
        "  ]\n",
        ")\n",
        "# Print the returned output from the LLM model\n",
        "print(completion.choices[0].message)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HN4lhYu72h_V",
        "outputId": "1f2d12ef-b6ea-493d-c77e-163ab666e066"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            "  \"role\": \"assistant\",\n",
            "  \"content\": \"Hello! How can I assist you today?\",\n",
            "  \"refusal\": null\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.2. Second Test: asking for capitals"
      ],
      "metadata": {
        "id": "qShtDitR5NUS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from openai import ChatCompletion\n",
        "\n",
        "resposta = ChatCompletion.create(\n",
        "    model=\"gpt-3.5-turbo\",\n",
        "    messages=[{\"role\": \"user\", \"content\": \"What is the capital of France? And Portugal?\"}]\n",
        ")\n",
        "print(resposta['choices'][0]['message']['content'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aOGwa_UB44Ma",
        "outputId": "2c11d7ed-8525-471d-9b03-9e2750141786"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The capital of France is Paris and the capital of Portugal is Lisbon.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. SECOND EXAMPLE: Adding a simple retrieval system\n",
        "\n",
        "In this example we the model to answer about Benfica games that happened in the lasts few months.\n",
        "First, we’ll ask the model a question without providing any context. And then, after giving it relevant data, we’ll ask the same question. We will see how the answer changes when the model has access to additional information through the retrieval system we created.\n"
      ],
      "metadata": {
        "id": "IFKHFpTeoBjK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# First of all we ask the model without context\n",
        "answer_without_context = openai.ChatCompletion.create(\n",
        "    model=\"gpt-3.5-turbo\",\n",
        "    messages=[{\"role\": \"user\", \"content\": \"What was the starting 11 of Benfica vs Porto on sunday 10/11/2024?\"}])\n",
        "\n",
        "print(answer_without_context['choices'][0]['message']['content'])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Luj2MFAS6odZ",
        "outputId": "7a59c503-0f95-4286-8d02-868674c41822"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I'm sorry, but as of now, I do not have the ability to access real-time sports data or information on specific matches. I recommend checking the official websites of Benfica and Porto or sports news websites for the most up-to-date information on the starting 11 for that match.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we want to implement the Retrieval-Augmented Generation (RAG) pipeline using document retrieval and augmenting the prompt with the context before passing it to the model.\n"
      ],
      "metadata": {
        "id": "SxhfOUeH-jGD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the function to search for relevant documents based on the query\n",
        "def search_relevant_documents(query, documents):\n",
        "    # using a simple keyword matching approach\n",
        "    key_words = query.lower().split()\n",
        "    relevant_documents = []\n",
        "\n",
        "    # Check the documents to find the keywords\n",
        "    # (we will create the documents later on the code)\n",
        "    for doc in documents:\n",
        "        if any(word in doc.lower() for word in key_words):\n",
        "            relevant_documents.append(doc)\n",
        "\n",
        "    return relevant_documents\n",
        "\n",
        "# Create the function to augment the prompt with our relevant context\n",
        "def augment_prompt(query, documents):\n",
        "    # Retrieve relevant documents for the query\n",
        "    docs_relevant = search_relevant_documents(query, documents)\n",
        "\n",
        "    # If there is no relevant documents, print a message\n",
        "    if docs_relevant:\n",
        "        context = \"\\n\".join(docs_relevant)\n",
        "    else:\n",
        "        context = \"No relevant documents found for this query.\"\n",
        "\n",
        "    # Format the augmented prompt adding the new information\n",
        "    augmented_prompt  = f\"\"\"Using the contexts below, answer the query.\n",
        "\n",
        "    Contexts:\n",
        "    {context}\n",
        "\n",
        "    Query: {query}\"\"\"\n",
        "    return augmented_prompt\n",
        "\n",
        "# Create documents\n",
        "documents = [\n",
        "    \"Starting eleven vs Porto, on 10/11/2024: Trubin/Bah,Araújo,Otamendi,Carreras/Florentino,Aursnes/Di María,Kokçu,Akturkoglu/Pavlidis\",\n",
        "    \"Starting eleven vs Bayern on 6/11/2024: Trubin/Kaboré,Araújo,Otamendi,Silva,Carreras/Renato,Aursnes,Kokçu/Akturkoglu,Amdouni\",\n",
        "    \"Starting eleven vs Farense on 2/11/2024: Trubin/Bah,Araújo,Otamendi,Carreras/Florentino,Aursnes/Di María,Kokçu,Akturkoglu/Pavlidis\",\n",
        "    \"Starting eleven vs Santa Clara on 17/8/2024: Trubin/Bah,Silva,Otamendi,Carreras/Renato,Aursnes/Amdouni,Beste,Akturkoglu/Cabral\"\n",
        "]\n",
        "\n",
        "# Ask the question\n",
        "query = \"What was the starting 11 of Benfica vs Farense?\"\n",
        "\n",
        "# Generate the augmented prompt to add in the conversation with the LLM\n",
        "augmented_prompt = augment_prompt(query, documents)\n",
        "\n",
        "# Ask the model with the augmented prompt\n",
        "answer_with_context = openai.ChatCompletion.create(\n",
        "    model=\"gpt-3.5-turbo\",\n",
        "    messages=[{\"role\": \"user\", \"content\": augmented_prompt}]\n",
        ")\n",
        "\n",
        "print(answer_with_context['choices'][0]['message']['content'])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ptOh3jfJ-ZYz",
        "outputId": "c29d26e3-903a-484e-cbb5-385217fa863a"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The starting eleven of Benfica vs Farense on 2/11/2024 was: Trubin/Bah, Araújo, Otamendi, Carreras/Florentino, Aursnes/Di María, Kokçu, Akturkoglu/Pavlidis.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. THIRD EXAMPLE: Integrating with an Operational Database for Querying\n",
        "\n",
        "Now, we show how to boost the model's capabilities by connecting it to an operational database. As we saw in the \"theory\", the model will now retrieve data from a live database, providing more accurate and up-to-date answers.\n",
        "\n",
        "We'll use a simple database setup and show how the model can query and incorporate live data into the answers.\n",
        "\n",
        "For this example we are using SQAlchemy, but there are several ways to integrate a LLM with a database.\n",
        "\n"
      ],
      "metadata": {
        "id": "IdiIIODOoRao"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1) SQLAlchemy Database Setup"
      ],
      "metadata": {
        "id": "AdEysNvoKQGH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import some necessary libraries\n",
        "from sqlalchemy import create_engine, Column, Integer, String, Date\n",
        "from sqlalchemy.ext.declarative import declarative_base\n",
        "from sqlalchemy.orm import sessionmaker\n",
        "import openai\n",
        "from datetime import date\n",
        "\n",
        "\n",
        "# Configure SQLAlchemy\n",
        "Base = declarative_base()\n",
        "\n",
        "# Define the class to create the 'games' dataset\n",
        "class Game(Base):\n",
        "    __tablename__ = 'games'\n",
        "\n",
        "    id = Column(Integer, primary_key=True)\n",
        "    opponent = Column(String, nullable=False)\n",
        "    date = Column(Date, nullable=False)\n",
        "    lineup = Column(String, nullable=False)\n",
        "\n",
        "# Create the connection to the SQLite database\n",
        "engine = create_engine('sqlite:///benfica_games.db')\n",
        "Base.metadata.create_all(engine)\n",
        "\n",
        "# Create the session to interact with the database\n",
        "Session = sessionmaker(bind=engine)\n",
        "session = Session()\n",
        "\n",
        "# Create the data about Benfica's matches (all the matches are added as instances of the Game class)\n",
        "games_data = [\n",
        "    Game(opponent='Porto', date=date(2024, 11, 10), lineup=\"Trubin/Bah,Araújo,Otamendi,Carreras/Florentino,Aursnes/Di María,Kokçu,Akturkoglu/Pavlidis\"),\n",
        "    Game(opponent='Bayern', date=date(2024, 11, 6), lineup=\"Trubin/Kaboré,Araújo,Otamendi,Silva,Carreras/Renato,Aursnes,Kokçu/Akturkoglu,Amdouni\"),\n",
        "    Game(opponent='Farense', date=date(2024, 11, 2), lineup=\"Trubin/Bah,Araújo,Otamendi,Carreras/Florentino,Aursnes/Di María,Kokçu,Akturkoglu/Pavlidis\"),\n",
        "    Game(opponent='Santa Clara', date=date(2024, 10, 30), lineup=\"Trubin/Bah,Silva,Otamendi,Carreras/Renato,Aursnes/Amdouni,Beste,Akturkoglu/Cabral\"),\n",
        "    Game(opponent='Rio Ave', date=date(2024, 10, 27), lineup=\"Trubin/Bah,Araújo,Otamendi,Carreras/Kokçu,Aursnes/Di María,Beste,Akturkoglu/Pavlidis\"),\n",
        "    Game(opponent='Feyenoord', date=date(2024, 10, 23), lineup=\"Trubin/Bah,Araújo,Otamendi,Carreras/Florentino,Aursnes/Di María,Kokçu,Akturkoglu/Pavlidis\"),\n",
        "    Game(opponent='Pevidem', date=date(2024, 10, 19), lineup=\"Soares/Kaboré,Araújo,Silva,Carreras/Florentino,Aursnes/Amdouni,Beste,Rolllheiser/Cabral\"),\n",
        "    Game(opponent='Atletico de Madrid', date=date(2024, 10, 2), lineup=\"Trubin/Bah,Araújo,Otamendi,Carreras/Florentino,Aursnes/Di María,Kokçu,Akturkoglu/Pavlidis\")\n",
        "]\n",
        "\n",
        "\n",
        "# Add the example data to the database (if not already added)\n",
        "if not session.query(Game).first():  # Avoid duplication\n",
        "    session.add_all(games_data)\n",
        "    session.commit()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZPPC2qpnKSzr",
        "outputId": "9c6220d7-0079-4c4b-d168-45fa314a9bd1"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-62-a791a4ee31ba>:10: MovedIn20Warning: The ``declarative_base()`` function is now available as sqlalchemy.orm.declarative_base(). (deprecated since: 2.0) (Background on SQLAlchemy 2.0 at: https://sqlalche.me/e/b8d9)\n",
            "  Base = declarative_base()\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2) Function to Search for relevant matches based on the query\n",
        "\n",
        "We create a function to search for relevant information in the database, augmenting the prompt with the starting lineup for the specific match asked in the query."
      ],
      "metadata": {
        "id": "vL4akmTfKU1L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to search for relevant matches in the database based on the query\n",
        "def search_relevant_games(query: str):\n",
        "    # Clean the query by removing the question mark (if present)\n",
        "    # The '?' can cause errors in the matching process\n",
        "    cleaned_query = query.lower().rstrip('?').strip()\n",
        "\n",
        "    # Split the query into keywords\n",
        "    keywords = cleaned_query.lower().split()\n",
        "\n",
        "    # Fetch all games from the database\n",
        "    games = session.query(Game).all()\n",
        "    relevant_games = []\n",
        "\n",
        "    # Check if the keywords (of the query) match the opponent field in the database\n",
        "    for game in games:\n",
        "        if any(keyword in game.opponent.lower() for keyword in keywords):\n",
        "            relevant_games.append(game)\n",
        "    return relevant_games\n",
        "\n",
        "# Function to augment the prompt with the database context\n",
        "def augment_prompt_from_db(query: str):\n",
        "    relevant_games = search_relevant_games(query)\n",
        "\n",
        "    # Format the context for the prompt\n",
        "    if relevant_games:\n",
        "        contexts = [\n",
        "            f\"Starting eleven vs {game.opponent} on {game.date}: {game.lineup}\"\n",
        "            for game in relevant_games\n",
        "        ]\n",
        "        context = \"\\n\".join(contexts)\n",
        "    else:\n",
        "        context = \"I couldn't find information about this game.\"\n",
        "\n",
        "    # Build the augmented prompt\n",
        "    augmented_prompt = f\"\"\"Using the contexts below, answer the query.\n",
        "\n",
        "    Contexts:\n",
        "    {context}\n",
        "\n",
        "    Query: {query}\"\"\"\n",
        "\n",
        "    return augmented_prompt\n"
      ],
      "metadata": {
        "id": "oTMK_BWEQk6D"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3) Query the Model with the Augmented Prompt\n",
        "\n",
        "We use the augmented prompt to query the model and get the answer based on the live database context."
      ],
      "metadata": {
        "id": "qp0Z4lzLKkJH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Query\n",
        "query = \"What was the starting 11 of Benfica vs Bayern?\"\n",
        "\n",
        "# Generate the augmented prompt from the database\n",
        "augmented_prompt = augment_prompt_from_db(query)\n",
        "\n",
        "# Ask the model with the augmented prompt\n",
        "response_with_context = openai.ChatCompletion.create(\n",
        "    model=\"gpt-3.5-turbo\",\n",
        "    messages=[\n",
        "        {\"role\": \"system\", \"content\": \"Here is information about Benfica's starting lineup for the specified matches.\"},\n",
        "        {\"role\": \"user\", \"content\": augmented_prompt}\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Print the answer\n",
        "print(response_with_context['choices'][0]['message']['content'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Hwht0aDRPIp",
        "outputId": "c87cbfc9-498c-4cef-eafd-09285529e8cc"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The starting eleven for Benfica against Bayern on 2024-11-06 was:\n",
            "- Trubin\n",
            "- Kaboré\n",
            "- Araújo\n",
            "- Otamendi\n",
            "- Silva\n",
            "- Carreras\n",
            "- Renato\n",
            "- Aursnes\n",
            "- Kokçu\n",
            "- Akturkoglu\n",
            "- Amdouni\n"
          ]
        }
      ]
    }
  ]
}